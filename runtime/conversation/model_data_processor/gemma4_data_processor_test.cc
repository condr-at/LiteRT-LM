// Copyright 2026 The ODML Authors.
//
// Licensed under the Apache License, Version 2.0 (the "License");
// you may not use this file except in compliance with the License.
// You may obtain a copy of the License at
//
//      http://www.apache.org/licenses/LICENSE-2.0
//
// Unless required by applicable law or agreed to in writing, software
// distributed under the License is distributed on an "AS IS" BASIS,
// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
// See the License for the specific language governing permissions and
// limitations under the License.

#include "runtime/conversation/model_data_processor/gemma4_data_processor.h"

#include <filesystem>  // NOLINT: Required for path manipulation.
#include <fstream>
#include <iterator>
#include <memory>
#include <optional>
#include <string>
#include <utility>
#include <variant>
#include <vector>

#include <gmock/gmock.h>
#include <gtest/gtest.h>
#include "absl/status/status.h"  // from @com_google_absl
#include "absl/status/statusor.h"  // from @com_google_absl
#include "absl/strings/str_cat.h"  // from @com_google_absl
#include "nlohmann/json.hpp"  // from @nlohmann_json
#include "runtime/components/prompt_template.h"
#include "runtime/components/sentencepiece_tokenizer.h"
#include "runtime/components/tokenizer.h"
#include "runtime/conversation/io_types.h"
#include "runtime/conversation/model_data_processor/gemma4_data_processor_config.h"
#include "runtime/engine/io_types.h"
#include "runtime/util/test_utils.h"  // NOLINT

namespace litert::lm {
namespace {

using json = nlohmann::ordered_json;
using ::testing::ElementsAre;
using ::testing::Eq;
using ::testing::status::IsOkAndHolds;

constexpr char kTestdataDir[] =
    "litert_lm/runtime/components/testdata/";

std::string GetTestdataPath(const std::string& file_name) {
  return (std::filesystem::path(::testing::SrcDir()) / kTestdataDir / file_name)
      .string();
}

absl::StatusOr<std::string> GetContents(const std::string& path) {
  std::ifstream input_stream(path);
  if (!input_stream.is_open()) {
    return absl::InternalError(absl::StrCat("Could not open file: ", path));
  }

  std::string content;
  content.assign((std::istreambuf_iterator<char>(input_stream)),
                 (std::istreambuf_iterator<char>()));
  return std::move(content);
}

MATCHER_P(HasInputText, text_input, "") {
  if (!std::holds_alternative<InputText>(arg)) {
    return false;
  }
  auto text_bytes = std::get<InputText>(arg).GetRawTextString();
  if (!text_bytes.ok()) {
    return false;
  }
  return text_bytes.value() == text_input->GetRawTextString().value();
}

class Gemma4DataProcessorTest : public ::testing::Test {
 protected:
  void SetUp() override {
    // TODO(b/483072440): This should be updated to use Gemma4 tokenizer before
    // make it public. Nano v4 has the same tokenizer as Nano v3.
    auto tokenizer = SentencePieceTokenizer::CreateFromFile(
        GetTestdataPath("nano_v3_sentencepiece.model"));
    ASSERT_OK(tokenizer);
    tokenizer_ = std::move(*tokenizer);
  }

  std::unique_ptr<Tokenizer> tokenizer_;
};

TEST_F(Gemma4DataProcessorTest, ToInputDataVectorTextOnly) {
  ASSERT_OK_AND_ASSIGN(auto processor, Gemma4DataProcessor::Create());
  const std::string rendered_template_prompt =
      "<ctrl99>user\ntest prompt\n<ctrl100>";
  const nlohmann::ordered_json messages = {
      {"role", "user"},
      {"content", "test prompt"},
  };
  ASSERT_OK_AND_ASSIGN(
      const std::vector<InputData> input_data,
      processor->ToInputDataVector(rendered_template_prompt, messages, {}));

  InputText expected_text("<ctrl99>user\ntest prompt\n<ctrl100>");
  EXPECT_THAT(input_data, ElementsAre(HasInputText(&expected_text)));
}

TEST_F(Gemma4DataProcessorTest, ToMessage) {
  ASSERT_OK_AND_ASSIGN(auto processor, Gemma4DataProcessor::Create());

  ASSERT_OK_AND_ASSIGN(
      const Message message,
      processor->ToMessage(Responses(TaskState::kProcessing, {"test response"}),
                           std::monostate{}));

  ASSERT_TRUE(std::holds_alternative<nlohmann::ordered_json>(message));
  const nlohmann::ordered_json& json_message =
      std::get<nlohmann::ordered_json>(message);
  EXPECT_EQ(
      json_message,
      json({{"role", "assistant"},
            {"content", {{{"type", "text"}, {"text", "test response"}}}}}));
}

TEST_F(Gemma4DataProcessorTest, ToMessageWithToolCalls) {
  Gemma4DataProcessorConfig config;
  JsonPreface preface{.tools = nlohmann::ordered_json::parse(
                          R"json([{
                            "name": "tool_name",
                            "parameters": {
                              "type": "object",
                              "properties": {
                                "x": {
                                  "type": "integer"
                                }
                              }
                            }
                          }])json")};

  ASSERT_OK_AND_ASSIGN(auto processor,
                       Gemma4DataProcessor::Create(config, preface));

  ASSERT_OK_AND_ASSIGN(
      const Message message,
      processor->ToMessage(Responses(TaskState::kProcessing,
                                     {"This is some text.\n"
                                      "<ctrl42>call:tool_name{x:1}<ctrl43>"
                                      "<ctrl42>call:tool_name{x:2}<ctrl43>"}),
                           std::monostate{}));

  ASSERT_TRUE(std::holds_alternative<nlohmann::ordered_json>(message));
  const nlohmann::ordered_json& json_message =
      std::get<nlohmann::ordered_json>(message);
  EXPECT_EQ(json_message, nlohmann::ordered_json::parse(R"json({
    "role": "assistant",
    "content": [
      {
        "type": "text",
        "text": "This is some text.\n"
      }
    ],
    "tool_calls": [
      {
        "type": "function",
        "function": {
          "name": "tool_name",
          "arguments": {
            "x": 1
          }
        }
      },
      {
        "type": "function",
        "function": {
          "name": "tool_name",
          "arguments": {
            "x": 2
          }
        }
      }
    ]
  })json"));
}

TEST_F(Gemma4DataProcessorTest, PromptTemplateToInputDataVectorTextOnly) {
  const std::string test_file_path =
      GetTestdataPath("google-gemini-nano-v4.jinja");
  ASSERT_OK_AND_ASSIGN(const std::string template_content,
                       GetContents(test_file_path));
  PromptTemplate prompt_template(template_content);

  const nlohmann::ordered_json messages = {
      {{"role", "system"}, {"content", "Hello world!"}},
      {{"role", "user"}, {"content", "How are you?"}},
      {{"role", "assistant"},
       {"content", "I am doing well, thanks for asking."}},
      {{"role", "user"}, {"content", "What is the capital of France?"}},
  };
  PromptTemplateInput template_input = {.messages = messages,
                                        .add_generation_prompt = true};

  ASSERT_OK_AND_ASSIGN(const std::string rendered_prompt,
                       prompt_template.Apply(template_input));

  ASSERT_OK_AND_ASSIGN(auto processor, Gemma4DataProcessor::Create());
  ASSERT_OK_AND_ASSIGN(
      const std::vector<InputData> input_data,
      processor->ToInputDataVector(rendered_prompt, messages, {}));
  InputText expected_text(R"(<ctrl99>system
Hello world!<ctrl100>
<ctrl99>user
How are you?<ctrl100>
<ctrl99>model
I am doing well, thanks for asking.<ctrl100>
<ctrl99>user
What is the capital of France?<ctrl100>
<ctrl99>model
)");
  EXPECT_THAT(input_data, ElementsAre(HasInputText(&expected_text)));
}

TEST_F(Gemma4DataProcessorTest, FormatTools) {
  ASSERT_OK_AND_ASSIGN(auto processor, Gemma4DataProcessor::Create());
  nlohmann::ordered_json tools = nlohmann::ordered_json::parse(R"json([
    {
      "name": "get_weather",
      "description": "Gets weather information.",
      "parameters": {
        "properties": {
          "location": {
            "type": "string",
            "description": "Weather location."
          }
        },
        "required": ["location"]
      }
    },
    {
      "name": "get_stock_price",
      "description": "Gets stock price.",
      "parameters": {
        "properties": {
          "symbol": {
            "type": "string",
            "description": "Stock symbol."
          }
        },
        "required": ["symbol"]
      }
    }
  ])json");

  ASSERT_OK_AND_ASSIGN(const nlohmann::ordered_json formatted_tools,
                       processor->FormatTools(tools));

  nlohmann::ordered_json expected = {
      ("declaration:get_weather{"
       "description:<ctrl46>Gets weather information.<ctrl46>,"
       "parameters:{"
       "properties:{"
       "location:{"
       "type:<ctrl46>STRING<ctrl46>,"
       "description:<ctrl46>Weather location.<ctrl46>"
       "}"   // location
       "},"  // properties
       "required:[<ctrl46>location<ctrl46>]"
       "}"  // parameters
       "}"  // declaration
       ),
      ("declaration:get_stock_price{"
       "description:<ctrl46>Gets stock price.<ctrl46>,"
       "parameters:{"
       "properties:{"
       "symbol:{"
       "type:<ctrl46>STRING<ctrl46>,"
       "description:<ctrl46>Stock symbol.<ctrl46>"
       "}"   // symbol
       "},"  // properties
       "required:[<ctrl46>symbol<ctrl46>]"
       "}"  // parameters
       "}"  // declaration
       )};
  EXPECT_EQ(formatted_tools, expected);
}

TEST_F(Gemma4DataProcessorTest, FormatToolsWithInvalidInput) {
  ASSERT_OK_AND_ASSIGN(auto processor, Gemma4DataProcessor::Create());
  // `tools` is not an array.
  nlohmann::ordered_json tools = nlohmann::ordered_json::parse(R"json({
    "name": "get_weather",
    "description": "Gets weather information.",
    "parameters": {
      "properties": {
        "location": {
          "type": "string",
          "description": "Weather location."
        }
      },
      "required": ["location"]
    }
  })json");

  EXPECT_THAT(processor->FormatTools(tools),
              testing::status::StatusIs(absl::StatusCode::kInvalidArgument));
}

TEST_F(Gemma4DataProcessorTest, MessageToTemplateInputWithStringContent) {
  ASSERT_OK_AND_ASSIGN(auto processor, Gemma4DataProcessor::Create());
  const nlohmann::ordered_json message = {
      {"role", "user"},
      {"content", "test prompt"},
  };

  // The template input is identical to the original message if the content is a
  // string.
  EXPECT_THAT(processor->MessageToTemplateInput(message),
              IsOkAndHolds(message));
}

TEST_F(Gemma4DataProcessorTest, MessageToTemplateInputWithTextContent) {
  ASSERT_OK_AND_ASSIGN(auto processor, Gemma4DataProcessor::Create());
  const nlohmann::ordered_json message = {
      {"role", "user"},
      {"content", {{{"type", "text"}, {"text", "test prompt"}}}},
  };

  // Text content items should be unchanged.
  EXPECT_THAT(processor->MessageToTemplateInput(message),
              IsOkAndHolds(message));
}

TEST_F(Gemma4DataProcessorTest, MessageToTemplateInputNoContent) {
  ASSERT_OK_AND_ASSIGN(auto processor, Gemma4DataProcessor::Create());
  const nlohmann::ordered_json message = {
      {"role", "user"},
  };

  // The template input should be the same as the original message if there is
  // no content or tool calls.
  EXPECT_THAT(processor->MessageToTemplateInput(message),
              IsOkAndHolds(message));
}

TEST_F(Gemma4DataProcessorTest, MessageToTemplateInputWithToolCalls) {
  ASSERT_OK_AND_ASSIGN(auto processor, Gemma4DataProcessor::Create());
  const nlohmann::ordered_json message = nlohmann::ordered_json::parse(R"json({
    "role": "assistant",
    "content": [
      {
        "type": "text",
        "text": "This is some text."
      }
    ],
    "tool_calls": [
      {
        "type": "function",
        "function": {
          "name": "tool1",
          "arguments": {
            "x": 1
          }
        }
      },
      {
        "type": "function",
        "function": {
          "name": "tool2",
          "arguments": {
            "y": "foo"
          }
        }
      }
    ]
  })json");

  EXPECT_THAT(processor->MessageToTemplateInput(message),
              IsOkAndHolds(nlohmann::ordered_json::parse(R"json({
    "role": "assistant",
    "content": [
      {
        "type": "text",
        "text": "This is some text."
      }
    ],
    "tool_calls": [
      {
        "type": "function",
        "function": {
          "name": "tool1",
          "arguments": {
            "x": "1"
          }
        }
      },
      {
        "type": "function",
        "function": {
          "name": "tool2",
          "arguments": {
            "y": "<ctrl46>foo<ctrl46>"
          }
        }
      }
    ]
  })json")));
}

TEST_F(Gemma4DataProcessorTest,
       MessageToTemplateInputWithToolResponsesNameAndValue) {
  ASSERT_OK_AND_ASSIGN(auto processor, Gemma4DataProcessor::Create());
  const nlohmann::ordered_json message = nlohmann::ordered_json::parse(R"json({
    "role": "tool",
    "content": [
      {
        "type": "tool_response",
        "tool_response": {
          "name": "tool_1",
          "value": {
            "key1": "value1",
            "key2": "value2"
          }
        }
      },
      {
        "type": "tool_response",
        "tool_response": {
          "name": "tool_2",
          "value": {
            "key3": "value3",
            "key4": "value4"
          }
        }
      }
    ]
  })json");

  // The tool responses should be formatted as text items with the tool name
  // and value converted to FC format.
  EXPECT_THAT(processor->MessageToTemplateInput(message),
              IsOkAndHolds(nlohmann::ordered_json::parse(R"json({
                "role": "tool",
                "content": [
                  {
                    "type": "text",
                    "text": "tool_1{key1:<ctrl46>value1<ctrl46>,key2:<ctrl46>value2<ctrl46>}"
                  },
                  {
                    "type": "text",
                    "text": "tool_2{key3:<ctrl46>value3<ctrl46>,key4:<ctrl46>value4<ctrl46>}"
                  }
                ]
              })json")));
}

TEST_F(Gemma4DataProcessorTest,
       MessageToTemplateInputWithToolResponseToolNameAndValue) {
  ASSERT_OK_AND_ASSIGN(auto processor, Gemma4DataProcessor::Create());
  const nlohmann::ordered_json message = nlohmann::ordered_json::parse(R"json({
    "role": "tool",
    "content": [
      {
        "type": "tool_response",
        "tool_response": {
          "tool_name": "tool_1",
          "value": {
            "key1": "value1"
          }
        }
      }
    ]
  })json");

  EXPECT_THAT(processor->MessageToTemplateInput(message),
              IsOkAndHolds(nlohmann::ordered_json::parse(R"json({
                "role": "tool",
                "content": [
                  {
                    "type": "text",
                    "text": "tool_1{key1:<ctrl46>value1<ctrl46>}"
                  }
                ]
              })json")));
}

TEST_F(Gemma4DataProcessorTest,
       MessageToTemplateInputWithToolResponseNameAndArgs) {
  ASSERT_OK_AND_ASSIGN(auto processor, Gemma4DataProcessor::Create());
  const nlohmann::ordered_json message = nlohmann::ordered_json::parse(R"json({
    "role": "tool",
    "content": [
      {
        "type": "tool_response",
        "tool_response": {
          "name": "tool_1",
          "key1": "value1"
        }
      }
    ]
  })json");

  EXPECT_THAT(processor->MessageToTemplateInput(message),
              IsOkAndHolds(nlohmann::ordered_json::parse(R"json({
                "role": "tool",
                "content": [
                  {
                    "type": "text",
                    "text": "tool_1{key1:<ctrl46>value1<ctrl46>}"
                  }
                ]
              })json")));
}

TEST_F(Gemma4DataProcessorTest,
       MessageToTemplateInputWithToolResponsesToolNameAndArgs) {
  ASSERT_OK_AND_ASSIGN(auto processor, Gemma4DataProcessor::Create());
  const nlohmann::ordered_json message = nlohmann::ordered_json::parse(R"json({
    "role": "tool",
    "content": [
      {
        "type": "tool_response",
        "tool_response": {
          "tool_name": "tool_1",
          "key1": "value1"
        }
      }
    ]
  })json");

  EXPECT_THAT(processor->MessageToTemplateInput(message),
              IsOkAndHolds(nlohmann::ordered_json::parse(R"json({
                "role": "tool",
                "content": [
                  {
                    "type": "text",
                    "text": "tool_1{key1:<ctrl46>value1<ctrl46>}"
                  }
                ]
              })json")));
}

TEST_F(Gemma4DataProcessorTest,
       MessageToTemplateInputWithToolResponseWithNonObjectValue) {
  ASSERT_OK_AND_ASSIGN(auto processor, Gemma4DataProcessor::Create());
  const nlohmann::ordered_json message = nlohmann::ordered_json::parse(R"json({
    "role": "tool",
    "content": [
      {
        "type": "tool_response",
        "tool_response": {
          "name": "tool_1",
          "value": "foo"
        }
      }
    ]
  })json");

  EXPECT_THAT(processor->MessageToTemplateInput(message),
              IsOkAndHolds(nlohmann::ordered_json::parse(R"json({
                "role": "tool",
                "content": [
                  {
                    "type": "text",
                    "text": "tool_1{value:<ctrl46>foo<ctrl46>}"
                  }
                ]
              })json")));
}

TEST_F(Gemma4DataProcessorTest,
       MessageToTemplateInputWithToolResponseWithNonObjectResponse) {
  ASSERT_OK_AND_ASSIGN(auto processor, Gemma4DataProcessor::Create());
  const nlohmann::ordered_json message = nlohmann::ordered_json::parse(R"json({
    "role": "tool",
    "content": [
      {
        "type": "tool_response",
        "tool_response": {
          "name": "tool_1",
          "response": "foo"
        }
      }
    ]
  })json");

  EXPECT_THAT(processor->MessageToTemplateInput(message),
              IsOkAndHolds(nlohmann::ordered_json::parse(R"json({
                "role": "tool",
                "content": [
                  {
                    "type": "text",
                    "text": "tool_1{response:<ctrl46>foo<ctrl46>}"
                  }
                ]
              })json")));
}

TEST_F(Gemma4DataProcessorTest, MessageToTemplateInputWithToolResponsesNoName) {
  ASSERT_OK_AND_ASSIGN(auto processor, Gemma4DataProcessor::Create());
  const nlohmann::ordered_json message = nlohmann::ordered_json::parse(R"json({
    "role": "tool",
    "content": [
      {
        "type": "tool_response",
        "tool_response": {
          "key1": "value1"
        }
      }
    ]
  })json");

  EXPECT_THAT(processor->MessageToTemplateInput(message),
              IsOkAndHolds(nlohmann::ordered_json::parse(R"json({
                "role": "tool",
                "content": [
                  {
                    "type": "text",
                    "text": "{key1:<ctrl46>value1<ctrl46>}"
                  }
                ]
              })json")));
}

TEST_F(Gemma4DataProcessorTest, MessageToTemplateInputWithToolContentAsObject) {
  ASSERT_OK_AND_ASSIGN(auto processor, Gemma4DataProcessor::Create());
  const nlohmann::ordered_json message = nlohmann::ordered_json::parse(R"json({
    "role": "tool",
    "content": {
      "name": "get_weather",
      "temperature": 72,
      "units": "Fahrenheit"
    }
  })json");

  EXPECT_THAT(processor->MessageToTemplateInput(message),
              IsOkAndHolds(nlohmann::ordered_json::parse(R"json({
                "role": "tool",
                "content": "get_weather{temperature:72,units:<ctrl46>Fahrenheit<ctrl46>}"
              })json")));
}

TEST_F(Gemma4DataProcessorTest,
       MessageToTemplateInputWithToolContentAsObjectWithNameAndResponse) {
  ASSERT_OK_AND_ASSIGN(auto processor, Gemma4DataProcessor::Create());
  const nlohmann::ordered_json message = nlohmann::ordered_json::parse(R"json({
    "role": "tool",
    "content": {
      "name": "tool_1",
      "response": {
        "key1": "value1"
      }
    }
  })json");

  EXPECT_THAT(processor->MessageToTemplateInput(message),
              IsOkAndHolds(nlohmann::ordered_json::parse(R"json({
                "role": "tool",
                "content": "tool_1{key1:<ctrl46>value1<ctrl46>}"
              })json")));
}

TEST_F(Gemma4DataProcessorTest,
       MessageToTemplateInputWithToolContentAsArrayWithNameAndResponse) {
  ASSERT_OK_AND_ASSIGN(auto processor, Gemma4DataProcessor::Create());
  const nlohmann::ordered_json message = nlohmann::ordered_json::parse(R"json({
    "role": "tool",
    "content": [
      {
        "name": "tool_1",
        "response": {
          "key1": "value1"
        }
      }
    ]
  })json");

  EXPECT_THAT(processor->MessageToTemplateInput(message),
              IsOkAndHolds(nlohmann::ordered_json::parse(R"json({
                "role": "tool",
                "content": [
                  {
                    "type": "text",
                    "text": "tool_1{key1:<ctrl46>value1<ctrl46>}"
                  }
                ]
              })json")));
}

TEST_F(Gemma4DataProcessorTest, MessageToTemplateInputWithToolContentAsString) {
  ASSERT_OK_AND_ASSIGN(auto processor, Gemma4DataProcessor::Create());
  const nlohmann::ordered_json message = nlohmann::ordered_json::parse(R"json({
    "role": "tool",
    "content": "get_weather{temperature:72,units:<ctrl46>Fahrenheit<ctrl46>}"
  })json");

  // String content should be kept as is.
  EXPECT_THAT(processor->MessageToTemplateInput(message),
              IsOkAndHolds(nlohmann::ordered_json::parse(R"json({
                "role": "tool",
                "content": "get_weather{temperature:72,units:<ctrl46>Fahrenheit<ctrl46>}"
              })json")));
}

struct RenderTemplateTestCase {
  std::string jinja_template_file;
  bool use_template_for_fc_format;
};

class Gemma4RenderTemplateTest
    : public Gemma4DataProcessorTest,
      public ::testing::WithParamInterface<RenderTemplateTestCase> {};

TEST_P(Gemma4RenderTemplateTest, RenderTemplateUserTurn) {
  const RenderTemplateTestCase& test_case = GetParam();

  // Load the prompt template.
  const std::string test_file_path =
      GetTestdataPath(test_case.jinja_template_file);
  ASSERT_OK_AND_ASSIGN(const std::string template_content,
                       GetContents(test_file_path));
  PromptTemplate prompt_template(template_content);

  // Create the message history.
  const nlohmann::ordered_json messages = nlohmann::ordered_json::parse(R"json([
    {
      "role": "user",
      "content":[
        {
          "type": "text",
          "text": "How is the weather in Paris and London?"
        }
      ]
    }
  ])json");

  // Create the model data processor.
  Gemma4DataProcessorConfig config;
  config.use_template_for_fc_format = test_case.use_template_for_fc_format;
  ASSERT_OK_AND_ASSIGN(auto processor, Gemma4DataProcessor::Create(config));

  // Convert the messages to template inputs.
  nlohmann::ordered_json message_template_input =
      nlohmann::ordered_json::array();
  for (const auto& message : messages) {
    ASSERT_OK_AND_ASSIGN(nlohmann::ordered_json input,
                         processor->MessageToTemplateInput(message));
    message_template_input.push_back(input);
  }

  // Render the template.
  PromptTemplateInput template_input = {.messages = message_template_input,
                                        .add_generation_prompt = true};
  ASSERT_OK_AND_ASSIGN(const std::string rendered_prompt,
                       prompt_template.Apply(template_input));

  // Compare to the expected prompt.
  EXPECT_EQ(rendered_prompt,
            "<ctrl99>user\n"
            "How is the weather in Paris and London?<ctrl100>\n"
            "<ctrl99>model\n");
}

TEST_P(Gemma4RenderTemplateTest, RenderTemplateAssistantTurnTextOnly) {
  const RenderTemplateTestCase& test_case = GetParam();

  // Load the prompt template.
  const std::string test_file_path =
      GetTestdataPath(test_case.jinja_template_file);
  ASSERT_OK_AND_ASSIGN(const std::string template_content,
                       GetContents(test_file_path));
  PromptTemplate prompt_template(template_content);

  // Create the message history.
  const nlohmann::ordered_json messages = nlohmann::ordered_json::parse(R"json([
    {
      "role": "user",
      "content":[
        {
          "type": "text",
          "text": "How is the weather in Paris and London?"
        }
      ]
    },
    {
      "role": "assistant",
      "content": [
        {
          "type": "text",
          "text": "Sorry, I can't help with that."
        }
      ]
    }
  ])json");

  // Create the model data processor.
  Gemma4DataProcessorConfig config;
  config.use_template_for_fc_format = test_case.use_template_for_fc_format;
  ASSERT_OK_AND_ASSIGN(auto processor, Gemma4DataProcessor::Create(config));

  // Convert the messages to template inputs.
  nlohmann::ordered_json message_template_input =
      nlohmann::ordered_json::array();
  for (const auto& message : messages) {
    ASSERT_OK_AND_ASSIGN(nlohmann::ordered_json input,
                         processor->MessageToTemplateInput(message));
    message_template_input.push_back(input);
  }

  // Render the template.
  PromptTemplateInput template_input = {.messages = message_template_input,
                                        .add_generation_prompt = false};
  ASSERT_OK_AND_ASSIGN(const std::string rendered_prompt,
                       prompt_template.Apply(template_input));

  // Compare to the expected prompt.
  EXPECT_EQ(rendered_prompt,
            "<ctrl99>user\n"
            "How is the weather in Paris and London?<ctrl100>\n"
            "<ctrl99>model\n"
            "Sorry, I can't help with that.<ctrl100>\n");
}

TEST_P(Gemma4RenderTemplateTest, RenderTemplateWithToolDeclarations) {
  const RenderTemplateTestCase& test_case = GetParam();

  // Load the prompt template.
  const std::string test_file_path =
      GetTestdataPath(test_case.jinja_template_file);
  ASSERT_OK_AND_ASSIGN(const std::string template_content,
                       GetContents(test_file_path));
  PromptTemplate prompt_template(template_content);

  nlohmann::ordered_json tools = nlohmann::ordered_json::parse(R"json([
    {
      "type": "function",
      "function": {
        "name": "get_weather",
        "description": "Gets weather information.",
        "parameters": {
          "properties": {
            "location": {
              "description": "Weather location.",
              "nullable": false,
              "type": "string"
            }
          },
          "required": ["location"],
          "type": "object"
        }
      }
    },
    {
      "type": "function",
      "function": {
        "name": "get_stock_price",
        "description": "Gets stock price.",
        "parameters": {
          "properties": {
            "symbol": {
              "description": "Stock symbol.",
              "nullable": false,
              "type": "string"
            }
          },
          "required": ["symbol"],
          "type": "object"
        }
      }
    }
  ])json");

  nlohmann::ordered_json messages = nlohmann::ordered_json::parse(R"json([
    {
      "role": "user",
      "content": "How is the weather in Paris and London?"
    }
  ])json");

  // Create the model data processor.
  Gemma4DataProcessorConfig config;
  config.use_template_for_fc_format = test_case.use_template_for_fc_format;
  ASSERT_OK_AND_ASSIGN(auto processor, Gemma4DataProcessor::Create(config));

  // Format the tools.
  ASSERT_OK_AND_ASSIGN(nlohmann::ordered_json formatted_tools,
                       processor->FormatTools(tools));

  // Render the template.
  PromptTemplateInput template_input = {.messages = messages,
                                        .tools = formatted_tools,
                                        .add_generation_prompt = true};
  ASSERT_OK_AND_ASSIGN(const std::string rendered_prompt,
                       prompt_template.Apply(template_input));

  // Compare to the expected prompt.
  EXPECT_THAT(rendered_prompt,
              Eq("<ctrl99>system\n"
                 "<ctrl40>"
                 "declaration:get_weather{"
                 "description:<ctrl46>Gets weather information.<ctrl46>,"
                 "parameters:{"
                 "properties:{"
                 "location:{"
                 "description:<ctrl46>Weather location.<ctrl46>,"
                 "nullable:false,"
                 "type:<ctrl46>STRING<ctrl46>"
                 "}"   // location
                 "},"  // properties
                 "required:[<ctrl46>location<ctrl46>],"
                 "type:<ctrl46>OBJECT<ctrl46>"
                 "}"  // parameters
                 "}"  // declaration
                 "<ctrl41>"
                 "<ctrl40>"
                 "declaration:get_stock_price{"
                 "description:<ctrl46>Gets stock price.<ctrl46>,"
                 "parameters:{"
                 "properties:{"
                 "symbol:{"
                 "description:<ctrl46>Stock symbol.<ctrl46>,"
                 "nullable:false,"
                 "type:<ctrl46>STRING<ctrl46>"
                 "}"   // symbol
                 "},"  // properties
                 "required:[<ctrl46>symbol<ctrl46>],"
                 "type:<ctrl46>OBJECT<ctrl46>"
                 "}"  // parameters
                 "}"  // declaration
                 "<ctrl41>"
                 "<ctrl100>\n"
                 "<ctrl99>user\n"
                 "How is the weather in Paris and London?<ctrl100>\n"
                 "<ctrl99>model\n"));
}

TEST_P(Gemma4RenderTemplateTest, RenderTemplateWithToolCalls) {
  const RenderTemplateTestCase& test_case = GetParam();

  // Load the prompt template.
  const std::string test_file_path =
      GetTestdataPath(test_case.jinja_template_file);
  ASSERT_OK_AND_ASSIGN(const std::string template_content,
                       GetContents(test_file_path));
  PromptTemplate prompt_template(template_content);

  // Create the message history.
  const nlohmann::ordered_json messages = nlohmann::ordered_json::parse(R"json([
    {
      "role": "user",
      "content":[
        {
          "type": "text",
          "text": "How is the weather in Paris and London?"
        }
      ]
    },
    {
      "role": "assistant",
      "tool_calls": [
        {
          "type": "function",
          "function": {
            "name": "get_weather",
            "arguments": {
              "location": "Paris"
            }
          }
        },
        {
          "type": "function",
          "function": {
            "name": "get_weather",
            "arguments": {
              "location": "London"
            }
          }
        }
      ]
    }
  ])json");

  // Create the model data processor.
  Gemma4DataProcessorConfig config;
  config.use_template_for_fc_format = test_case.use_template_for_fc_format;
  ASSERT_OK_AND_ASSIGN(auto processor, Gemma4DataProcessor::Create(config));

  // Convert the messages to template inputs.
  nlohmann::ordered_json message_template_input =
      nlohmann::ordered_json::array();
  for (const auto& message : messages) {
    ASSERT_OK_AND_ASSIGN(nlohmann::ordered_json input,
                         processor->MessageToTemplateInput(message));
    message_template_input.push_back(input);
  }

  // Render the template.
  PromptTemplateInput template_input = {.messages = message_template_input,
                                        .add_generation_prompt = false};
  ASSERT_OK_AND_ASSIGN(const std::string rendered_prompt,
                       prompt_template.Apply(template_input));

  // Compare to the expected prompt.
  //
  // Note that a model turn containing tool calls is terminated by
  // "<ctrl44>" instead of "<ctrl100>".
  EXPECT_EQ(rendered_prompt,
            "<ctrl99>user\n"
            "How is the weather in Paris and London?<ctrl100>\n"
            "<ctrl99>model\n"
            "<ctrl42>"
            "call:get_weather{location:<ctrl46>Paris<ctrl46>}"
            "<ctrl43>"
            "<ctrl42>"
            "call:get_weather{location:<ctrl46>London<ctrl46>}"
            "<ctrl43>"
            "<ctrl44>");
}

TEST_P(Gemma4RenderTemplateTest, RenderTemplateWithToolResponses) {
  const RenderTemplateTestCase& test_case = GetParam();

  // Load the prompt template.
  const std::string test_file_path =
      GetTestdataPath(test_case.jinja_template_file);
  ASSERT_OK_AND_ASSIGN(const std::string template_content,
                       GetContents(test_file_path));
  PromptTemplate prompt_template(template_content);

  // Create the message history.
  const nlohmann::ordered_json messages = nlohmann::ordered_json::parse(R"json([
    {
      "role": "user",
      "content":[
        {
          "type": "text",
          "text": "How is the weather in Paris and London?"
        }
      ]
    },
    {
      "role": "assistant",
      "tool_calls": [
        {
          "type": "function",
          "function": {
            "name": "get_weather",
            "arguments": {
              "location": "Paris"
            }
          }
        },
        {
          "type": "function",
          "function": {
            "name": "get_weather",
            "arguments": {
              "location": "London"
            }
          }
        }
      ]
    },
    {
      "role": "tool",
      "content": [
        {
          "name": "get_weather",
          "response": {
            "location": "Paris",
            "temperature": 20,
            "unit": "C",
            "weather": "Sunny"
          }
        },
        {
          "name": "get_weather",
          "response": {
            "location": "London",
            "temperature": 15,
            "unit": "C",
            "weather": "Cloudy"
          }
        }
      ]
    }
  ])json");

  // Create the model data processor.
  Gemma4DataProcessorConfig config;
  config.use_template_for_fc_format = test_case.use_template_for_fc_format;
  ASSERT_OK_AND_ASSIGN(auto processor, Gemma4DataProcessor::Create(config));

  // Convert the messages to template inputs.
  nlohmann::ordered_json message_template_input =
      nlohmann::ordered_json::array();
  for (const auto& message : messages) {
    ASSERT_OK_AND_ASSIGN(nlohmann::ordered_json input,
                         processor->MessageToTemplateInput(message));
    message_template_input.push_back(input);
  }

  // Render the template.
  PromptTemplateInput template_input = {.messages = message_template_input,
                                        .add_generation_prompt = true};
  ASSERT_OK_AND_ASSIGN(const std::string rendered_prompt,
                       prompt_template.Apply(template_input));

  // Compare to the expected prompt.
  //
  // Note that the generation prompt is suppressed after the tool response,
  // despite add_generation_prompt = true.
  EXPECT_EQ(rendered_prompt,
            "<ctrl99>user\n"
            "How is the weather in Paris and London?<ctrl100>\n"
            "<ctrl99>model\n"
            "<ctrl42>"
            "call:get_weather{location:<ctrl46>Paris<ctrl46>}"
            "<ctrl43>"
            "<ctrl42>"
            "call:get_weather{location:<ctrl46>London<ctrl46>}"
            "<ctrl43>"
            "<ctrl44>"
            "response:get_weather{"
            "location:<ctrl46>Paris<ctrl46>,"
            "temperature:20,"
            "unit:<ctrl46>C<ctrl46>,"
            "weather:<ctrl46>Sunny<ctrl46>"
            "}"  // response:get_weather
            "<ctrl45>"
            "<ctrl44>"
            "response:get_weather{"
            "location:<ctrl46>London<ctrl46>,"
            "temperature:15,"
            "unit:<ctrl46>C<ctrl46>,"
            "weather:<ctrl46>Cloudy<ctrl46>"
            "}"  // response:get_weather
            "<ctrl45>");
}

TEST_P(Gemma4RenderTemplateTest, RenderTemplateWithMultipleToolMessages) {
  const RenderTemplateTestCase& test_case = GetParam();

  // Load the prompt template.
  const std::string test_file_path =
      GetTestdataPath(test_case.jinja_template_file);
  ASSERT_OK_AND_ASSIGN(const std::string template_content,
                       GetContents(test_file_path));
  PromptTemplate prompt_template(template_content);

  // Create the message history.
  const nlohmann::ordered_json messages = nlohmann::ordered_json::parse(R"json([
    {
      "role": "user",
      "content":[
        {
          "type": "text",
          "text": "How is the weather in Paris and London?"
        }
      ]
    },
    {
      "role": "assistant",
      "tool_calls": [
        {
          "type": "function",
          "function": {
            "name": "get_weather",
            "arguments": {
              "location": "Paris"
            }
          }
        },
        {
          "type": "function",
          "function": {
            "name": "get_weather",
            "arguments": {
              "location": "London"
            }
          }
        }
      ]
    },
    {
      "role": "tool",
      "content": {
        "name": "get_weather",
        "response": {
          "location": "Paris",
          "temperature": 20,
          "unit": "C",
          "weather": "Sunny"
        }
      }
    },
    {
      "role": "tool",
      "content": {
        "name": "get_weather",
        "response": {
          "location": "London",
          "temperature": 15,
          "unit": "C",
          "weather": "Cloudy"
        }
      }
    }
  ])json");

  // Create the model data processor.
  Gemma4DataProcessorConfig config;
  config.use_template_for_fc_format = test_case.use_template_for_fc_format;
  ASSERT_OK_AND_ASSIGN(auto processor, Gemma4DataProcessor::Create(config));

  // Convert the messages to template inputs.
  nlohmann::ordered_json message_template_input =
      nlohmann::ordered_json::array();
  for (const auto& message : messages) {
    ASSERT_OK_AND_ASSIGN(nlohmann::ordered_json input,
                         processor->MessageToTemplateInput(message));
    message_template_input.push_back(input);
  }

  // Render the template.
  PromptTemplateInput template_input = {.messages = message_template_input,
                                        .add_generation_prompt = true};
  ASSERT_OK_AND_ASSIGN(const std::string rendered_prompt,
                       prompt_template.Apply(template_input));

  // Compare to the expected prompt.
  //
  // Note that the generation prompt is suppressed after the tool response,
  // despite add_generation_prompt = true.
  EXPECT_EQ(rendered_prompt,
            "<ctrl99>user\n"
            "How is the weather in Paris and London?<ctrl100>\n"
            "<ctrl99>model\n"
            "<ctrl42>"
            "call:get_weather{location:<ctrl46>Paris<ctrl46>}"
            "<ctrl43>"
            "<ctrl42>"
            "call:get_weather{location:<ctrl46>London<ctrl46>}"
            "<ctrl43>"
            "<ctrl44>"
            "response:get_weather{"
            "location:<ctrl46>Paris<ctrl46>,"
            "temperature:20,"
            "unit:<ctrl46>C<ctrl46>,"
            "weather:<ctrl46>Sunny<ctrl46>"
            "}"  // response:get_weather
            "<ctrl45>"
            "<ctrl44>"
            "response:get_weather{"
            "location:<ctrl46>London<ctrl46>,"
            "temperature:15,"
            "unit:<ctrl46>C<ctrl46>,"
            "weather:<ctrl46>Cloudy<ctrl46>"
            "}"  // response:get_weather
            "<ctrl45>");
}

TEST_P(Gemma4RenderTemplateTest,
       RenderTemplateWithModelResponseAfterToolResponse) {
  const RenderTemplateTestCase& test_case = GetParam();

  // Load the prompt template.
  const std::string test_file_path =
      GetTestdataPath(test_case.jinja_template_file);
  ASSERT_OK_AND_ASSIGN(const std::string template_content,
                       GetContents(test_file_path));
  PromptTemplate prompt_template(template_content);

  // Create the message history.
  const nlohmann::ordered_json messages = nlohmann::ordered_json::parse(R"json([
    {
      "role": "user",
      "content":[
        {
          "type": "text",
          "text": "How is the weather in Paris and London?"
        }
      ]
    },
    {
      "role": "assistant",
      "tool_calls": [
        {
          "type": "function",
          "function": {
            "name": "get_weather",
            "arguments": {
              "location": "Paris"
            }
          }
        },
        {
          "type": "function",
          "function": {
            "name": "get_weather",
            "arguments": {
              "location": "London"
            }
          }
        }
      ]
    },
    {
      "role": "tool",
      "content": [
        {
          "name": "get_weather",
          "response": {
            "location": "Paris",
            "temperature": 20,
            "unit": "C",
            "weather": "Sunny"
          }
        },
        {
          "name": "get_weather",
          "response": {
            "location": "London",
            "temperature": 15,
            "unit": "C",
            "weather": "Cloudy"
          }
        }
      ]
    },
    {
      "role": "assistant",
      "content": [
        {
          "type": "text",
          "text": "The weather in Paris is sunny and the weather in London is cloudy."
        }
      ]
    }
  ])json");

  // Create the model data processor.
  Gemma4DataProcessorConfig config;
  config.use_template_for_fc_format = test_case.use_template_for_fc_format;
  ASSERT_OK_AND_ASSIGN(auto processor, Gemma4DataProcessor::Create(config));

  // Convert the messages to template inputs.
  nlohmann::ordered_json message_template_input =
      nlohmann::ordered_json::array();
  for (const auto& message : messages) {
    ASSERT_OK_AND_ASSIGN(nlohmann::ordered_json input,
                         processor->MessageToTemplateInput(message));
    message_template_input.push_back(input);
  }

  // Render the template.
  PromptTemplateInput template_input = {.messages = message_template_input,
                                        .add_generation_prompt = false};
  ASSERT_OK_AND_ASSIGN(const std::string rendered_prompt,
                       prompt_template.Apply(template_input));

  // Compare to the expected prompt.
  EXPECT_EQ(rendered_prompt,
            "<ctrl99>user\n"
            "How is the weather in Paris and London?<ctrl100>\n"
            "<ctrl99>model\n"
            "<ctrl42>"
            "call:get_weather{location:<ctrl46>Paris<ctrl46>}"
            "<ctrl43>"
            "<ctrl42>"
            "call:get_weather{location:<ctrl46>London<ctrl46>}"
            "<ctrl43>"
            "<ctrl44>"
            "response:get_weather{"
            "location:<ctrl46>Paris<ctrl46>,"
            "temperature:20,"
            "unit:<ctrl46>C<ctrl46>,"
            "weather:<ctrl46>Sunny<ctrl46>"
            "}"  // response:get_weather
            "<ctrl45>"
            "<ctrl44>"
            "response:get_weather{"
            "location:<ctrl46>London<ctrl46>,"
            "temperature:15,"
            "unit:<ctrl46>C<ctrl46>,"
            "weather:<ctrl46>Cloudy<ctrl46>"
            "}"  // response:get_weather
            "<ctrl45>"
            "The weather in Paris is sunny and the weather in London is cloudy."
            "<ctrl100>\n");
}

TEST_P(Gemma4RenderTemplateTest, RenderTemplateWithEmptyAssistantMessage) {
  const RenderTemplateTestCase& test_case = GetParam();

  // Load the prompt template.
  const std::string test_file_path =
      GetTestdataPath(test_case.jinja_template_file);
  ASSERT_OK_AND_ASSIGN(const std::string template_content,
                       GetContents(test_file_path));
  PromptTemplate prompt_template(template_content);

  // Create the message history.
  const nlohmann::ordered_json messages = nlohmann::ordered_json::parse(R"json([
    {
      "role": "user",
      "content":[
        {
          "type": "text",
          "text": "How is the weather in Paris?"
        }
      ]
    },
    {
      "role": "assistant",
      "tool_calls": [
        {
          "type": "function",
          "function": {
            "name": "get_weather",
            "arguments": {
              "location": "Paris"
            }
          }
        }
      ]
    },
    {
      "role": "tool",
      "content": [
        {
          "name": "get_weather",
          "response": {
            "location": "Paris",
            "temperature": 20,
            "unit": "C",
            "weather": "Sunny"
          }
        }
      ]
    },
    {
      "role": "assistant"
    },
    {
      "role": "user",
      "content":[
        {
          "type": "text",
          "text": "How is the weather in New York?"
        }
      ]
    }
  ])json");

  // Create the model data processor.
  Gemma4DataProcessorConfig config;
  config.use_template_for_fc_format = test_case.use_template_for_fc_format;
  ASSERT_OK_AND_ASSIGN(auto processor, Gemma4DataProcessor::Create(config));

  // Convert the messages to template inputs.
  nlohmann::ordered_json message_template_input =
      nlohmann::ordered_json::array();
  for (const auto& message : messages) {
    ASSERT_OK_AND_ASSIGN(nlohmann::ordered_json input,
                         processor->MessageToTemplateInput(message));
    message_template_input.push_back(input);
  }

  // Render the template.
  PromptTemplateInput template_input = {.messages = message_template_input,
                                        .add_generation_prompt = true};
  ASSERT_OK_AND_ASSIGN(const std::string rendered_prompt,
                       prompt_template.Apply(template_input));

  // Compare to the expected prompt.
  EXPECT_EQ(rendered_prompt,
            "<ctrl99>user\n"
            "How is the weather in Paris?<ctrl100>\n"
            "<ctrl99>model\n"
            "<ctrl42>"
            "call:get_weather{location:<ctrl46>Paris<ctrl46>}"
            "<ctrl43>"
            "<ctrl44>"
            "response:get_weather{"
            "location:<ctrl46>Paris<ctrl46>,"
            "temperature:20,"
            "unit:<ctrl46>C<ctrl46>,"
            "weather:<ctrl46>Sunny<ctrl46>"
            "}"  // response:get_weather
            "<ctrl45>"
            "<ctrl100>\n"
            "<ctrl99>user\n"
            "How is the weather in New York?<ctrl100>\n"
            "<ctrl99>model\n");
}

INSTANTIATE_TEST_SUITE_P(
    FcFormatCodeOrTemplate, Gemma4RenderTemplateTest,
    testing::ValuesIn<RenderTemplateTestCase>({
        {.jinja_template_file = "google-gemini-nano-v4.jinja",
         .use_template_for_fc_format = false},
    }));

}  // namespace
}  // namespace litert::lm
